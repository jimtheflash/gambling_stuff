---
title: "Time to Build Some Fucking NBA Models"
author: "Jim Kloet"
date: "3/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Purpose

We're trying to make a lot of money betting on sports, right? Right. So the purpose of this is to build some fucking NBA models that generate a lot of revenue.

So I guess to be more specific, there are a few different kinds of wagers that we can try to win using predictive modeling techniques:

* Conventional Bets: Moneylines, Spread, Totals
  + We've modeled this stuff before, and there's probably not a huge edge to be gained, but in cases where a book is running a promo or something, or in cases where it's possible to get different odds for alternative bets (e.g. better odds for larger margins of victory, or worse odds for smaller totals) then there could be some edges to be found.
  + Per Nate Silver, these kinds of bets are best modeled using a combination of team-level and player-level models, which I think makes sense: there are macro team things happening (coaching strategy, roster composition, schedule/travel, etc.) and then micro player things happening (individual player performance, interactions between players) that get aggregated upwards.
  
* Game Props: First Team to Score, First FG, Make/Miss First Shots...

* Player Props: Expected Points, Rebounds, Assists, Made 3's...

I'm making this in `RMarkdown` so that I can be expository outside of comments. BUT I'M GONNA BE EXPOSITORY IN THE COMMENTS TOO!

## Load Stuff

We're doing this in `tidyverse` ~~to spite Shook~~ cuz it's a nice framework, and also to try and grab the attention of the fine folks at RStudio in case they're looking for someone to try to liaise with sports folk, though tbqh they already have those connections and more. AHEM.

```{r load-stuff}

# Here's the thing - I'm still going to explicitly specify which library non-base functions come from using the `::` operator; that'll make it a lot easier for me to packagize later. But I'm still loading the libraries here, if for no other reason, in case I forget where a function comes from.

## tidyverse, but IRL we'll only load the specific packages we need (which yes, dependencies, but c'mon we can manage a few dependencies)
# install.packages('tidyverse')
library(tidyverse)

## tidymodels cuz that's the framework we're using, but I bet there's gonna be a bunch of stuff we need to install...
# install.packages('tidymodels')
library(tidymodels)

## slider to handle some rolling stuff
# install.packages('slider')
library(slider)

# also lets start a timer
t1 <- Sys.time()
```

All these conflicts are another good reason to explicitly specify which package you're using for a given function.

## NBA Team-Game Models

These models are going to predict team-games, so that we can estimate conventional bets, and also so we can mix team-level estimates with player-level estimates to get the best of everything.

I'm gonna make a slew of custom functions for recursive and windowed functions.

```{r tg-engineering-prep}

# make an incrementor function to use with purrr::accumulate later
incrementor <- function(prev, new, growth = 1) {
  dplyr::if_else(new == 0|is.na(new), new, prev + growth)
}

# also should probably make the rolling functions ahead of time, iirc it moves a lot faster that way
rollsum_p <- function(x, p, ...) {
  slider::slide_vec(
    x,
    sum,
    .before = p,
    ...
  )
}

rollmean_p <- function(x, p, ...) {
  slider::slide_vec(
    x,
    mean,
    .before = p,
    ...
  )
}

rollmax_p <- function(x, p, ...) {
  slider::slide_vec(
    x,
    max,
    .before = p,
    ...
  )
}

rollmin_p <- function(x, p, ...) {
  slider::slide_vec(
    x,
    min,
    .before = p,
    ...
  )
}

rollmedian_p <- function(x, p, ...) {
  slider::slide_vec(
    x,
    median,
    .before = p,
    ...
  )
}

rollvar_p <- function(x, p, ...) {
  slider::slide_vec(
    x,
    var,
    .before = p,
    ...
  )
}

rollcount_days_p <- function(x, date_index, p, ...) {
  slider::slide_period_vec(
    x,
    date_index,
    "day",
    dplyr::n_distinct,
    .before = p,
    ...
  )
}

## OH, you can specify the list of funs as purrr formulae for dplyr::across in advance!
## TIME TO GO NUTS Y'ALL; not including the sums cuz that's gonna be a special case, and since all the windows are the same size then there's no reason to include both a sum and a mean/median
rolling_list <- list(
  # means
  rollmean_3 = ~rollmean_p(.x, 3, .complete = TRUE),
  rollmean_5 = ~rollmean_p(.x, 5, .complete = TRUE),
  rollmean_7 = ~rollmean_p(.x, 7, .complete = TRUE),
  rollmean_11 = ~rollmean_p(.x, 11, .complete = TRUE),
  rollmean_13 = ~rollmean_p(.x, 13, .complete = TRUE),
  rollmean_17 = ~rollmean_p(.x, 17, .complete = TRUE),
  rollmean_19 = ~rollmean_p(.x, 19, .complete = TRUE),
  rollmean_23 = ~rollmean_p(.x, 23, .complete = TRUE),
  # maxes
  rollmax_3 = ~rollmax_p(.x, 3, .complete = TRUE),
  rollmax_5 = ~rollmax_p(.x, 5, .complete = TRUE),
  rollmax_7 = ~rollmax_p(.x, 7, .complete = TRUE),
  rollmax_11 = ~rollmax_p(.x, 11, .complete = TRUE),
  rollmax_13 = ~rollmax_p(.x, 13, .complete = TRUE),
  rollmax_17 = ~rollmax_p(.x, 17, .complete = TRUE),
  rollmax_19 = ~rollmax_p(.x, 19, .complete = TRUE),
  rollmax_23 = ~rollmax_p(.x, 23, .complete = TRUE),
  # mins
  rollmin_3 = ~rollmin_p(.x, 3, .complete = TRUE),
  rollmin_5 = ~rollmin_p(.x, 5, .complete = TRUE),
  rollmin_7 = ~rollmin_p(.x, 7, .complete = TRUE),
  rollmin_11 = ~rollmin_p(.x, 11, .complete = TRUE),
  rollmin_13 = ~rollmin_p(.x, 13, .complete = TRUE),
  rollmin_17 = ~rollmin_p(.x, 17, .complete = TRUE),
  rollmin_19 = ~rollmin_p(.x, 19, .complete = TRUE),
  rollmin_23 = ~rollmin_p(.x, 23, .complete = TRUE),
  # medians
  rollmedian_3 = ~rollmedian_p(.x, 3, .complete = TRUE),
  rollmedian_5 = ~rollmedian_p(.x, 5, .complete = TRUE),
  rollmedian_7 = ~rollmedian_p(.x, 7, .complete = TRUE),
  rollmedian_11 = ~rollmedian_p(.x, 11, .complete = TRUE),
  rollmedian_13 = ~rollmedian_p(.x, 13, .complete = TRUE),
  rollmedian_17 = ~rollmedian_p(.x, 17, .complete = TRUE),
  rollmedian_19 = ~rollmedian_p(.x, 19, .complete = TRUE),
  rollmedian_23 = ~rollmedian_p(.x, 23, .complete = TRUE),
  # vars
  rollvar_3 = ~rollvar_p(.x, 3, .complete = TRUE),
  rollvar_5 = ~rollvar_p(.x, 5, .complete = TRUE),
  rollvar_7 = ~rollvar_p(.x, 7, .complete = TRUE),
  rollvar_11 = ~rollvar_p(.x, 11, .complete = TRUE),
  rollvar_13 = ~rollvar_p(.x, 13, .complete = TRUE),
  rollvar_17 = ~rollvar_p(.x, 17, .complete = TRUE),
  rollvar_19 = ~rollvar_p(.x, 19, .complete = TRUE),
  rollvar_23 = ~rollvar_p(.x, 23, .complete = TRUE)
)

rollsum_list <- list(
  rollsum_3 = ~rollsum_p(.x, 3, .complete = TRUE),
  rollsum_5 = ~rollsum_p(.x, 5, .complete = TRUE),
  rollsum_7 = ~rollsum_p(.x, 7, .complete = TRUE),
  rollsum_11 = ~rollsum_p(.x, 11, .complete = TRUE),
  rollsum_13 = ~rollsum_p(.x, 13, .complete = TRUE),
  rollsum_17 = ~rollsum_p(.x, 17, .complete = TRUE),
  rollsum_19 = ~rollsum_p(.x, 19, .complete = TRUE),
  rollsum_23 = ~rollsum_p(.x, 23, .complete = TRUE)
  )

# rollcount_list <- list(
#         last3days = ~rollcount_days_p(game_id, game_date, 3, .complete = TRUE),
#         last5days = ~rollcount_days_p(game_id, game_date, 5, .complete = TRUE),
#         last7days = ~rollcount_days_p(game_id, game_date, 7, .complete = TRUE),
#         last11days = ~rollcount_days_p(game_id, game_date, 11, .complete = TRUE),
#         last13days = ~rollcount_days_p(game_id, game_date, 13, .complete = TRUE),
#         last17days = ~rollcount_days_p(game_id, game_date, 17, .complete = TRUE),
#         last19days = ~rollcount_days_p(game_id, game_date, 19, .complete = TRUE),
#         last23days = ~rollcount_days_p(game_id, game_date, 23, .complete = TRUE)
#         )

```

Helper functions in hand, we can start engineering our data for preprocessing.

``` {r tg-engineering}
# first we gotta read in some data; how about, the 2018-19 season: pre-pandemic! completed season! We'll filter to regular season in a moment
player_games <- read.csv('../data/nba_gamelogs/nba_gamelogs_2016-17.csv',
                         colClasses = 'character') %>%
  dplyr::bind_rows(
    read.csv('../data/nba_gamelogs/nba_gamelogs_2017-18.csv',
                         colClasses = 'character')) %>%
  dplyr::bind_rows(
    read.csv('../data/nba_gamelogs/nba_gamelogs_2018-19.csv',
                         colClasses = 'character')) %>%
  dplyr::bind_rows(
    read.csv('../data/nba_gamelogs/nba_gamelogs_2019-20.csv',
             colClasses = 'character')) %>%
  dplyr::bind_rows(
    read.csv('../data/nba_gamelogs/nba_gamelogs_2020-21.csv',
             colClasses = 'character')) %>%
  identity()

# this is at the player-game level, so we'll have to roll it up to team-game, and tidy it up a bit to get opponent data and that sorta stuff

# ok now time to engineer some data
tidy_playergames <- player_games %>%
  janitor::clean_names() %>%
  dplyr::mutate(home_away = dplyr::if_else(grepl('@', matchup), 'away', 'home'),
                join_var = dplyr::if_else(home_away == 'away', 'home', 'away'),
                strata_var = paste0(home_away, wl)) %>%
  dplyr::mutate(
    dplyr::across(
      c(min, pts, fga, fgm, fta, ftm, pf, ast, reb, tov),
      as.numeric
    ))

team_games1 <- tidy_playergames %>%
  dplyr::filter(season_type == 'Regular Season') %>%
  dplyr::group_by(season_year, team_name, game_id, home_away, strata_var, join_var, wl) %>%
  dplyr::summarise(
    game_date = min(as.Date(game_date)),
    total_minutes = sum(min, na.rm = TRUE), # we're going to use this to count overtimes later on
    players_in_rotation = dplyr::n_distinct(player_id[min > 0]),
    dplyr::across(
      c(pts, fga, fgm, fta, ftm, pf, ast, reb, tov),
      sum,
      .names = '{.col}_game'
      ),
    .groups = 'keep'
    ) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(season_year, team_name, game_date) %>%
  dplyr::group_by(season_year, team_name) %>%
  dplyr::mutate(
    dplyr::across(
      ends_with('_game'),
      rolling_list)) %>%
  dplyr::ungroup()

# idk if i'll need this? guess it doesn't hurt to stash
# wide_home_away_games <- team_games1 %>%
#   tidyr::pivot_wider(
#     id_cols = c(game_id),
#     names_from = home_away,
#     values_from = dplyr::ends_with('_game'),
#     names_glue = "{home_away}_{.value}"
#   )

team_games2 <- team_games1 %>%
  dplyr::inner_join(
    team_games1 %>%
      dplyr::select(-strata_var, -join_var, -total_minutes, -wl, -season_year) %>%
      dplyr::rename(opp_name = team_name),
    by = c('game_id' = 'game_id',
           'game_date' = 'game_date',
           'join_var' = 'home_away')) %>%
  dplyr::select(-join_var) %>%
  dplyr::ungroup()

# fix the names to be a little clearer what they actually are (there'll still have to be some kind of explicit documentation about this)
fixed_names <- gsub('.x', '', names(team_games2), fixed = TRUE)
fixed_names <- gsub('.y', '_allowed', fixed_names, fixed = TRUE)
fixed_names <- sub('players_in_rotation_allowed', 'opp_players_in_rotation', fixed_names)
names(team_games2) <- fixed_names

# engineer some important stuff like margin of victory, overtimes, b2b stuff, etc.
team_games3 <- team_games2 %>%
  dplyr::mutate(
    score_margin_game = pts_game - pts_game_allowed,
    overtime_game = dplyr::if_else(total_minutes > 240, TRUE, FALSE),
    count_overtimes_game = dplyr::case_when(
      !overtime_game ~ 0,
      overtime_game ~ (total_minutes - 240) / 25)) %>%
  dplyr::select(-total_minutes) %>%
  dplyr::arrange(season_year, team_name, game_date) %>%
  dplyr::group_by(season_year, team_name) %>%
  dplyr::mutate(
    b2b_g1 = dplyr::case_when(dplyr::lead(game_date) == game_date + 1 ~ TRUE, 
                              dplyr::lead(game_date) != game_date + 1 ~ FALSE,
                              row_number() == max(row_number()) ~ FALSE),
    b2b_g2 = dplyr::case_when(dplyr::lag(game_date) == game_date - 1 ~ TRUE, 
                              dplyr::lag(game_date) != game_date - 1 ~ FALSE,
                              row_number() == 1 ~ FALSE)) %>%
  # windows
  dplyr::mutate(
    dplyr::across(
      score_margin_game,
      rolling_list)) %>%
  # sums
  dplyr::mutate(
    win = as.numeric(wl == 'W'),
    lose = as.numeric(wl == 'L'),
    home = as.numeric(home_away == 'home'),
    away = as.numeric(home_away == 'away'),
    dplyr::across(
      c(win, lose, home, away),
      rollsum_list)) %>%
  # streaks
  dplyr::mutate(
    dplyr::across(
      c(win, lose, home, away),
      list(streak = ~accumulate(.x, incrementor)))) %>%
  dplyr::mutate(
    dplyr::across(
      game_id,
      list(
        last3days = ~rollcount_days_p(.x, game_date, 3, .complete = TRUE),
        last5days = ~rollcount_days_p(.x, game_date, 5, .complete = TRUE),
        last7days = ~rollcount_days_p(.x, game_date, 7, .complete = TRUE),
        last11days = ~rollcount_days_p(.x, game_date, 11, .complete = TRUE),
        last13days = ~rollcount_days_p(.x, game_date, 13, .complete = TRUE),
        last17days = ~rollcount_days_p(.x, game_date, 17, .complete = TRUE),
        last19days = ~rollcount_days_p(.x, game_date, 19, .complete = TRUE),
        last23days = ~rollcount_days_p(.x, game_date, 23, .complete = TRUE)
        ))) %>%
  dplyr::ungroup()

# add the opponent data to make this stupid wide, but then we'll know how opponents are playing offense and defense
team_games4 <- team_games3 %>%
  dplyr::inner_join(
    team_games3 %>%
      dplyr::select(-season_year, -game_date, -home_away, -strata_var, -wl, -dplyr::matches('players_in_rotation')), 
    by = c('game_id' = 'game_id', 
           'team_name' = 'opp_name')
  ) 

# fix the names again
fixed_names2 <- gsub('.x', '', names(team_games4), fixed = TRUE)
fixed_names2 <- sapply(fixed_names2, function(x) ifelse(grepl('.y', x, fixed = TRUE), paste0('opp_', x), x))
fixed_names2 <- gsub('.y', '', fixed_names2, fixed = TRUE)
names(team_games4) <- fixed_names2  

# and now the real kicker - we gotta use data from date n to predict outcomes in game n+1; gonna have to lag a bunch of shit and remove unlagged versions
team_games5 <- team_games4 %>%
  dplyr::arrange(season_year, team_name, game_date) %>%
  dplyr::group_by(season_year, team_name) %>%
  dplyr::mutate(
    dplyr::across(
      dplyr::matches('_game|_allowed|_roll|_streak|_rotation'),
      list(
        lagged = ~dplyr::lag(.x)
      )
    )
  ) %>%
  dplyr::mutate(
    pts_game_outcome = pts_game,
    pts_game_allowed_outcome = pts_game_allowed,
    score_margin_game_outcome = score_margin_game,
    win_outcome = factor(win)
  ) %>%
  dplyr::select(
    -win, -lose, -opp_win, -opp_lose, -wl,
    -dplyr::ends_with('_allowed'),
    -dplyr::ends_with('_game'),
    -dplyr::ends_with('streak'),
    -dplyr::ends_with('rotation'),
    -dplyr::matches('_[1-9]?.$')
  ) %>%
  dplyr::ungroup()

# # ADD THE ODDS
# odds <- read.csv('../data/nba_lines/tidy_lines.csv')
# 
# team_games6 <- team_games5 %>%
#   dplyr::inner_join(odds %>% select(-X, -NBA_team_abbrev, -team) %>% mutate(game_date = as.Date(game_date)),
#                     by = c("team_name" = "NBA_team_name",
#                            "game_date" = "game_date",
#                            "home_away" = "home_away"))

# for this model, we're nuking everything with NAs, which means we'll only start predicting for games that happen after the 23rd game (since that's the longest rolling window we're using); if we take a reduced models approach (i think we should!) then this is gonna mean making at least n models where n is the number of different rolling windows...

# TODO: omitting the odds here, consider uncommenting the odds part and swapping the below with team_games6
to_model <- team_games5 %>%
  na.omit() %>%
  dplyr::mutate(
    dplyr::across(
      tidyselect:::where(is.logical),
      as.numeric
    )
  ) %>%
  # # this mutate should happen elsewhere in the pipeline?
  # dplyr::mutate(
  #   season_team_name = paste0(season_year, team_name),
  #   season_opp_team_name = paste0(season_year, opp_team_name)
  # ) %>%
  dplyr::select(-opp_name) %>%
  dplyr::ungroup()

message('we have ', ncol(to_model), ' variables for modeling and ', nrow(to_model), ' observations for the modeling process')  
```

Now that the data are engineered, we can start feeding to the `tidymodels` framework for pre-processing. A few things we should be considering, and I'm not sure the best approach tbqh:

* should there only be one observation per game_id?
* can same games live in the test and training sets? is that fair?


``` {r start-cooking}

# so the split is important here - we don't want any of the same game_ids (or do we? actually idk...)

train_test_split <- rsample::initial_split(to_model, prop = 4/5, strata = strata_var)
team_games_train <- rsample::training(train_test_split)
team_games_test <- rsample::testing(train_test_split)

# so now we build a recipe
rec <- recipes::recipe(team_games_train, . ~ .) %>%
  recipes::update_role(rsample::everything(), new_role = 'predictor') %>%
  recipes::update_role(strata_var, new_role = 'splitting') %>%
  recipes::update_role(game_id, season_year, team_name, opp_team_name, new_role = 'id') %>%
  recipes::update_role(pts_game_outcome, win_outcome, score_margin_game_outcome, pts_game_allowed_outcome, new_role = 'unused_outcomes') %>%
  recipes::step_date(game_date, features = "dow") %>%
  recipes::step_date(game_date, features = "month") %>%
  recipes::step_holiday(game_date) %>%
  recipes::step_rm(game_date) %>%
  recipes::step_dummy(recipes::all_predictors(), -recipes::all_numeric(), one_hot = TRUE) %>%
  recipes::step_naomit(recipes::all_predictors()) %>%
  recipes::step_zv(recipes::all_predictors()) %>%
  recipes::step_nzv(recipes::all_predictors(), freq_cut = 4999, unique_cut = .0002) %>%
  recipes::step_lincomb(recipes::all_predictors()) %>%
  recipes::step_corr(recipes::all_predictors(), threshold = 0.95) %>%
  recipes::prep()


# bake it to see the final input data for your model
baked <- recipes::bake(rec, team_games_train)

## NOT RUN
# View(skimr::skim(baked))
message('now we have ', ncol(baked), ' variables and ', nrow(baked), ' observations for training')

```

Now we can build some models using `parsnip` and tune 'em using `tune` and pipeline 'em using `workflows` and all sorts of other nifty shit.

``` {r build-models}

# we're gonna cross-validate here
cv_folds <- rsample::vfold_cv(team_games_train, v = 3,
                             strata = strata_var)
# and sometimes bootstrap
boots <- rsample::bootstraps(team_games_train, times = 3,
                             strata = strata_var)

# # parallelize
# cores <- parallel::detectCores()
# #Create cluster with desired number of cores, leave one open for the machine         
# #core processes
# cl <- doParallel::makeCluster(cores[1]-1)
# #Register cluster
# doParallel::registerDoParallel(cl)

# control settings for simualted annealing tuning
sim_anneal_ctrl <- finetune::control_sim_anneal(
  no_improve = 10L,
  restart = 3,
  cooling_coef = .3,
  verbose = TRUE, 
  save_pred = TRUE, 
  save_workflow = TRUE
)

#### lasso ####
st0 <- Sys.time()

lasso_reg_mod <- parsnip::linear_reg(mode = 'regression',
                                     penalty = tune::tune(),
                                     mixture = 1) %>%
  parsnip::set_engine('glmnet')


##### pts_for #####
pts_for_rec <- rec %>%
  recipes::update_role(pts_game_outcome, new_role = 'outcome')

pts_for_wf <- workflows::workflow() %>%
  workflows::add_model(lasso_reg_mod) %>%
  workflows::add_recipe(pts_for_rec)

pts_for_tg <- try(
  finetune::tune_sim_anneal(
    pts_for_wf,
    resamples = cv_folds,
    iter = 100,
    initial = 3,
    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq),
    control = sim_anneal_ctrl
  ), silent = TRUE)

pts_for_final_params <- pts_for_tg %>%
  tune::select_best("rmse")

pts_for_final_wf <- pts_for_wf %>%
  tune::finalize_workflow(pts_for_final_params)

pts_for_final_mod <- pts_for_final_wf %>%
  parsnip::fit(team_games_train)

pts_for_preds <- predict(pts_for_final_mod, team_games_test)

# coefs
pts_for_final_mod %>%
  workflows::pull_workflow_fit() %>%
  yardstick::tidy() %>%
  View()

# compute the rsq
tibble(bind_cols(truth = team_games_test$pts_game_outcome, pts_for_preds)) %>%
  yardstick::rsq(truth, .pred)


##### pts_allowed #####
pts_allowed_rec <- rec %>%
  recipes::update_role(pts_game_allowed_outcome, new_role = 'outcome')

pts_allowed_wf <- workflows::workflow() %>%
  workflows::add_model(lasso_reg_mod) %>%
  workflows::add_recipe(pts_allowed_rec)

pts_allowed_tg <- try(
  finetune::tune_sim_anneal(
    pts_allowed_wf,
    resamples = cv_folds,
    iter = 100,
    initial = 3,
    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq),
    control = sim_anneal_ctrl
  ), silent = TRUE)

pts_allowed_final_params <- pts_allowed_tg %>%
  tune::select_best("rmse")

pts_allowed_final_wf <- pts_allowed_wf %>%
  tune::finalize_workflow(pts_allowed_final_params)

pts_allowed_final_mod <- pts_allowed_final_wf %>%
  parsnip::fit(team_games_train)

# preds
pts_allowed_preds <- predict(pts_allowed_final_mod, team_games_test)

# get the coefs to est variable importance
pts_allowed_final_mod %>%
  workflows::pull_workflow_fit() %>%
  yardstick::tidy() %>%
  View()

# compute the rsq
tibble(bind_cols(truth = team_games_test$pts_game_allowed_outcome, pts_allowed_preds)) %>%
  yardstick::rsq(truth, .pred)

#### win % ####

lasso_logreg_mod <- parsnip::logistic_reg(mode = 'classification',
                                          penalty = tune::tune(),
                                          mixture = 1) %>%
  parsnip::set_engine('glmnet')

win_rec <- rec %>%
  recipes::update_role(win_outcome, new_role = 'outcome')

win_wf <- workflows::workflow() %>%
  workflows::add_model(lasso_logreg_mod) %>%
  workflows::add_recipe(win_rec)

win_tg <- try(
  finetune::tune_sim_anneal(
    win_wf,
    resamples = cv_folds,
    iter = 100,
    initial = 3,
    metrics = yardstick::metric_set(yardstick::roc_auc, yardstick::bal_accuracy),
    control = sim_anneal_ctrl
  ), silent = TRUE)

win_final_params <- win_tg %>%
  tune::select_best("roc_auc")

win_final_wf <- win_wf %>%
  tune::finalize_workflow(win_final_params)

win_final_mod <- win_final_wf %>%
  parsnip::fit(team_games_train)

win_final_mod %>%
  workflows::pull_workflow_fit() %>%
  yardstick::tidy() %>%
  View()

win_preds <- predict(win_final_mod, team_games_test, type = 'prob')

# make an roc curve plot
tibble(bind_cols(truth = team_games_test$win_outcome, win_preds)) %>%
  mutate(truth = forcats::fct_relevel(truth, '1', '0')) %>%
  yardstick::roc_curve(truth, .pred_1,
                       options = list(smooth = TRUE)) %>%
  autoplot()

et0 <- Sys.time()
et0 - st0

```

OK, these work, but the rest of them are funky, so breaking into 2 chunks for easier construction of reprexes.


``` {r other-models}
# #### lgbm ####
# remotes::install_github("curso-r/treesnip")
# st1 <- Sys.time()
# library(treesnip)
# 
# lgbm_params <- dials::parameters(
#   dials::finalize(dials::mtry(), team_games_train),
#   dials::trees(),
#   dials::tree_depth(),
#   dials::learn_rate()
# )
# 
# lgbm_mod <- parsnip::boost_tree(mode = 'regression',
#                                 mtry = tune::tune(),
#                                 trees = tune::tune(),
#                                 tree_depth = tune::tune(),
#                                 learn_rate = tune::tune()
#                                 ) %>%
#   parsnip::set_engine('lightgbm')
# 
# lgbm_workflow <- workflows::workflow() %>%
#   workflows::add_model(lgbm_mod) %>%
#   workflows::add_recipe(rec)
# 
# lgbm_tg <- lgbm_workflow %>%
#     tune::tune_grid(
#     resamples = cv_folds,
#     param_info = lgbm_params,
#     grid = 10,
#     metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq),
#     control = stacks::control_stack_grid()
#   )
# 
# et1 <- Sys.time()
# et1 - st1

#### xgboost ####
# 
# st2 <- Sys.time()
# 
# xgb_params <- dials::parameters(
#   dials::finalize(dials::mtry(), team_games_train),
#   dials::trees(),
#   dials::min_n(),
#   dials::tree_depth(),
#   dials::learn_rate(),
#   dials::loss_reduction())
# 
# xgb_mod <- parsnip::boost_tree(mode = 'classification',
#                                mtry = tune::tune(),
#                                trees = tune::tune(),
#                                min_n = tune::tune(),
#                                tree_depth = tune::tune(),
#                                learn_rate = tune::tune(),
#                                loss_reduction = tune::tune()) %>%
#   parsnip::set_engine('xgboost')
# 
# xgb_workflow <- workflows::workflow() %>%
#   workflows::add_model(xgb_mod) %>%
#   workflows::add_recipe(win_rec)
# 
# xgb_tg <- try(
#   finetune::tune_sim_anneal(
#     xgb_workflow,
#     resamples = cv_folds,
#     iter = 100,
#     initial = 3,
#     param_info = xgb_params,
#     metrics = yardstick::metric_set(yardstick::roc_auc, yardstick::bal_accuracy),
#     control = sim_anneal_ctrl
#   ), silent = TRUE)
# 
# xgb_final_params <- xgb_tg %>%
#   tune::select_best("roc_auc")
# 
# xgb_final_wf <- xgb_workflow %>%
#   tune::finalize_workflow(xgb_final_params)
# 
# xgb_final_mod <- xgb_final_wf %>%
#   parsnip::fit(team_games_train)
# 
# xgb_final_mod %>%
#   workflows::pull_workflow_fit() %>%
#   vip::vip(geom = 'point')
# 
# et2 <- Sys.time()
# et2 - st2
#
# # tune::collect_metrics(xgb_tg)
# # xgb_fit <- xgb_workflow %>%
# #   parsnip::fit(data = team_games_train)
# 
# #### ranger ####
# 
# st3 <- Sys.time()
# 
# TODO: find a way to get *some* aspect of a rf to run
# rf_mod <- parsnip::rand_forest(mode = 'regression') %>%
#   parsnip::set_engine('ranger')
# 
# rf_workflow <- workflows::workflow() %>%
#   workflows::add_model(rf_mod) %>%
#   workflows::add_recipe(rec)
# 
# rf_tg <- rf_workflow %>%
#   tune::fit_resamples(
#     resamples = cv_folds,
#     metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq),
#     control = tune::control_resamples(verbose = TRUE, save_workflow = TRUE, save_pred = TRUE)
#   )
# 
# rf_params <- dials::parameters(
#   dials::finalize(dials::mtry(c(15, 150)), team_games_train),
#   dials::trees(),
#   dials::min_n()
# )
# rf_mod <- parsnip::rand_forest(mode = 'regression',
#                                mtry = tune::tune(),
#                                trees = tune::tune(),
#                                min_n = tune::tune()) %>%
#   parsnip::set_engine('randomForest')
# 
# rf_workflow <- workflows::workflow() %>%
#   workflows::add_model(rf_mod) %>%
#   workflows::add_recipe(rec)
# 
# rf_tg <- try(
#   finetune::tune_sim_anneal(
#     rf_workflow,
#     resamples = cv_folds,
#     iter = 100,
#     initial = 3,
#     param_info = rf_params,
#     metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq),
#     control = sim_anneal_ctrl
#   ), silent = TRUE)
# et3 <- Sys.time()
# et3 - st3
# 
# tune::collect_metrics(rf_tg)
# rf_fit <- rf_workflow %>%
#   parsnip::fit(data = team_games_train)
# 
# # stack em up!
# remotes::install_github("tidymodels/stacks", ref = "main")
# 
# st <- stacks::stacks() %>%
#   stacks::add_candidates(lasso_tg) %>%
#   stacks::add_candidates(ridge_tg)
# 
# %>%
#   stacks::add_candidates(xgb_tg) 
# 
# %>%
#   stacks::add_candidates(rf_tg)
# 
# st_blended_preds <- st %>%
#   stacks::blend_predictions()
# 
# st_fitted <- st_blended_preds %>%
#   stacks::fit_members()
# 
# test_preds <- predict(st_fitted, team_games_test)
# test_outcome <- team_games_test$pts_game_outcome
# 
#  qplot(test_preds$.pred, test_outcome)
#  cor(test_preds$.pred, test_outcome)
# TODO: 
# get model performance stuff calculated i think with the yardstick package
# get some earth models tuned up
# GET THAT TENSORFLOWWWWWWWWWWWWWW ROLLING
# setup some stacks using the stacks package
# and then start functionalizing and packagizing so we can dockerize and automate ðŸ˜
t2 <- Sys.time()

t2-t1
```


## NBA Player-Game Models

So is this gonna be a per-minute sort of thing? Probably a per-minute sort of thing.

```{r}
# 
# mods <- tidy_playergames %>%
#   na.omit() %>%
#   lm(pts ~ min, data = .)
# 
# 
# player_team_dfs <- tidy_playergames %>%
#   group_by(player_name, team_name, season_year) %>%
#   group_split()

```

